{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1UmttqtpCt8HEhEj2tiP3wbu09kyFTQxj","authorship_tag":"ABX9TyMnOI5ipdY7aunDQfegcuPv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k_5uZP7hgE2c","executionInfo":{"status":"ok","timestamp":1705648405646,"user_tz":0,"elapsed":129691,"user":{"displayName":"daniel ejiofor","userId":"04973055555148086008"}},"outputId":"6f6a91a3-6b54-495d-aa4b-13100627cb4c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unzipping completed.\n"]}],"source":["import zipfile\n","\n","# Path to the zip file\n","zip_file_path = '/content/drive/MyDrive/Colab Notebooks/archive.zip'\n","\n","# Path where you want to extract the contents of the zip file\n","extract_to_path = '/content/drive/MyDrive/Colab Notebooks/'\n","\n","# Opening the zip file in read mode\n","with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","    # Extracting all the contents into the directory specified\n","    zip_ref.extractall(extract_to_path)\n","\n","print(\"Unzipping completed.\")\n"]},{"cell_type":"code","source":["train_dir = '/content/drive/MyDrive/Colab Notebooks/ultrasound breast classification/train'\n","val_dir = '/content/drive/MyDrive/Colab Notebooks/ultrasound breast classification/val'\n"],"metadata":{"id":"C2Y8S7_khb5W","executionInfo":{"status":"ok","timestamp":1705648882165,"user_tz":0,"elapsed":181,"user":{"displayName":"daniel ejiofor","userId":"04973055555148086008"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torchvision.models import vit_b_16  # Import ViT\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import StepLR\n","\n","# Define your transforms\n","train_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # Resize the image to the size required by ViT\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","val_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# Load datasets\n","train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n","val_dataset = datasets.ImageFolder(root=val_dir, transform=val_transform)\n","\n","# Create data loaders\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32)\n","\n","# Load the pre-trained Vision Transformer (ViT) model\n","model = vit_b_16(pretrained=True)\n","\n","# Modify the classifier to fit binary classification\n","model.heads.head = nn.Linear(model.heads.head.in_features, 2)\n","\n","# Define loss function, optimizer, and scheduler\n","criterion = nn.CrossEntropyLoss()\n","optimizer = AdamW(model.parameters(), lr=1e-4)\n","scheduler = StepLR(optimizer, step_size=7, gamma=0.1)  # Adjust step_size and gamma as appropriate\n","\n","# Fine-tuning loop\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","criterion = criterion.to(device)\n","\n","num_epochs = 5  # Set number of epochs to 5\n","\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    for images, labels in train_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","        _, predicted = torch.max(outputs, 1)\n","        correct_predictions += (predicted == labels).sum().item()\n","        total_predictions += labels.size(0)\n","\n","    scheduler.step()\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    epoch_accuracy = correct_predictions / total_predictions\n","    print(f'Epoch {epoch+1}/{num_epochs} Train loss: {epoch_loss:.4f} Accuracy: {epoch_accuracy:.4f}')\n","\n","    # Validation loop\n","    model.eval()\n","    val_running_loss = 0.0\n","    val_correct_predictions = 0\n","    val_total_predictions = 0\n","\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            val_running_loss += loss.item() * images.size(0)\n","            _, predicted = torch.max(outputs, 1)\n","            val_correct_predictions += (predicted == labels).sum().item()\n","            val_total_predictions += labels.size(0)\n","\n","    val_epoch_loss = val_running_loss / len(val_loader.dataset)\n","    val_epoch_accuracy = val_correct_predictions / val_total_predictions\n","    print(f'Epoch {epoch+1}/{num_epochs} Validation loss: {val_epoch_loss:.4f} Accuracy: {val_epoch_accuracy:.4f}')\n","\n","\n","# Save the model checkpoint\n","torch.save(model.state_dict(), 'vit_transformer_finetuned.pth')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9UchpVd_lAql","executionInfo":{"status":"ok","timestamp":1705650426102,"user_tz":0,"elapsed":1540551,"user":{"displayName":"daniel ejiofor","userId":"04973055555148086008"}},"outputId":"120fe597-89c5-4edb-85e8-6674ee1304f4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n","100%|██████████| 330M/330M [00:02<00:00, 116MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5 Train loss: 0.2195 Accuracy: 0.9096\n","Epoch 1/5 Validation loss: 0.3702 Accuracy: 0.8633\n","Epoch 2/5 Train loss: 0.0669 Accuracy: 0.9764\n","Epoch 2/5 Validation loss: 0.1748 Accuracy: 0.9378\n","Epoch 3/5 Train loss: 0.0437 Accuracy: 0.9840\n","Epoch 3/5 Validation loss: 0.2706 Accuracy: 0.9322\n","Epoch 4/5 Train loss: 0.0300 Accuracy: 0.9900\n","Epoch 4/5 Validation loss: 0.3524 Accuracy: 0.9089\n","Epoch 5/5 Train loss: 0.0399 Accuracy: 0.9846\n","Epoch 5/5 Validation loss: 0.2024 Accuracy: 0.9433\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torchvision.models import vit_b_16  # Import ViT\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import StepLR\n","\n","# Define your transforms\n","train_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # Resize the image to the size required by ViT\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","val_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# Load datasets\n","train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n","val_dataset = datasets.ImageFolder(root=val_dir, transform=val_transform)\n","\n","# Create data loaders\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32)\n","\n","# Load the pre-trained Vision Transformer (ViT) model\n","model = vit_b_16(pretrained=True)\n","\n","# Modify the classifier to fit binary classification, with dropout for regularization\n","model.heads.head = nn.Sequential(\n","    nn.Dropout(p=0.5),  # Add dropout with 50% probability\n","    nn.Linear(model.heads.head.in_features, 2)\n",")\n","\n","# Define loss function, optimizer, and scheduler\n","criterion = nn.CrossEntropyLoss()\n","optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)  # Added weight decay for regularization\n","scheduler = StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","# Fine-tuning loop with early stopping\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","criterion = criterion.to(device)\n","\n","num_epochs = 5\n","best_val_loss = float('inf')\n","patience = 2  # Number of epochs to wait after last time validation loss improved.\n","patience_counter = 0\n","\n","for epoch in range(num_epochs):\n","    # Training loop\n","    model.train()\n","    running_loss = 0.0\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    for images, labels in train_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","        _, predicted = torch.max(outputs, 1)\n","        correct_predictions += (predicted == labels).sum().item()\n","        total_predictions += labels.size(0)\n","\n","    scheduler.step()\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    epoch_accuracy = correct_predictions / total_predictions\n","    print(f'Epoch {epoch+1}/{num_epochs} Train loss: {epoch_loss:.4f} Accuracy: {epoch_accuracy:.4f}')\n","\n","    # Validation loop\n","    model.eval()\n","    val_running_loss = 0.0\n","    val_correct_predictions = 0\n","    val_total_predictions = 0\n","\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            val_running_loss += loss.item() * images.size(0)\n","            _, predicted = torch.max(outputs, 1)\n","            val_correct_predictions += (predicted == labels).sum().item()\n","            val_total_predictions += labels.size(0)\n","\n","    val_epoch_loss = val_running_loss / len(val_loader.dataset)\n","    val_epoch_accuracy = val_correct_predictions / val_total_predictions\n","    print(f'Epoch {epoch+1}/{num_epochs} Validation loss: {val_epoch_loss:.4f} Accuracy: {val_epoch_accuracy:.4f}')\n","\n","\n","    # Early stopping check\n","    if val_epoch_loss < best_val_loss:\n","        best_val_loss = val_epoch_loss\n","        patience_counter = 0\n","        torch.save(model.state_dict(), 'best_vit_model.pth')\n","    else:\n","        patience_counter += 1\n","        if patience_counter >= patience:\n","            print(f\"Early stopping triggered at epoch {epoch+1}\")\n","            break\n","\n","# Load the best model\n","model.load_state_dict(torch.load('best_vit_model.pth'))\n","\n","# Print final results or perform further actions\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HlHqnQ0BtE0E","executionInfo":{"status":"ok","timestamp":1705652447828,"user_tz":0,"elapsed":1547070,"user":{"displayName":"daniel ejiofor","userId":"04973055555148086008"}},"outputId":"d5decc8c-941a-4ab2-e01d-77214d1d3ea9"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5 Train loss: 0.2106 Accuracy: 0.9110\n","Epoch 1/5 Validation loss: 0.2353 Accuracy: 0.9078\n","Epoch 2/5 Train loss: 0.1206 Accuracy: 0.9547\n","Epoch 2/5 Validation loss: 0.2129 Accuracy: 0.9278\n","Epoch 3/5 Train loss: 0.0481 Accuracy: 0.9836\n","Epoch 3/5 Validation loss: 0.1254 Accuracy: 0.9600\n","Epoch 4/5 Train loss: 0.0509 Accuracy: 0.9806\n","Epoch 4/5 Validation loss: 0.1600 Accuracy: 0.9378\n","Epoch 5/5 Train loss: 0.0418 Accuracy: 0.9850\n","Epoch 5/5 Validation loss: 0.2029 Accuracy: 0.9467\n","Early stopping triggered at epoch 5\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":6}]}]}